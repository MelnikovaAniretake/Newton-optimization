{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы безусловной глобальной оптимизации. Метод Ньютона для оптимизации. Метод BGFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизация (от латинского слова «optimus» – наилучший) – поиск наилучшего варианта, при наличии множества альтернативных. Всевозможные устройства, процессы и ситуации, применительно к которым предстоит решать задачу оптимизации, называют объектом оптимизации. \n",
    "\n",
    "Методы оптимизации занимаются построением оптимальных решений для математических моделей, при этом именно вид модели определяет метод или методы, используемые для построения оптимального решения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Классификация задач оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу оптимизации: найти минимум функции f(x) n-мерного аргумента x, компоненты которого удовлетворяют системе ограничений в виде уравнений или неравенств.Такая задача называется задачей условной оптимизации. Если задача не содержит ограничения и рассматривается на всем пространстве, то это задача безусловной оптимизации.                                                                                  \n",
    "\n",
    "Задачи оптимизации классифицируются в соответствии с видом функций и размерностью вектора х.                                                                                                              \n",
    "\n",
    "Задачи без ограничений с N=1 называются задачами одномерной оптимизации, с N=2 и N>2 – многомерной оптимизации.                                                                                                    Если в задаче функции линейны, то это задача с линейными ограничениями. При этом целевая функция f(x) может быть как линейной, так и нелинейной.  Задача условной оптимизации, в которой все функции линейны, называется задачей линейного программирования. \n",
    "\n",
    "Задачи с нелинейной целевой функцией называются задачами нелинейного программирования. При этом если f(x) квадратичная функция, то задачей квадратичного программирования. Если f(x) отношение линейных функций, а ограничения – линейные, то рассматривается задача дробнолинейного программирования.                                                                                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Классификация методов оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В соответствии с классификацией задач оптимизации классифицируются и методы оптимизации. Методы одномерной оптимизации (нахождение наименьшего значения функции f(x) в области определения D(f) и точек, в которых это значение достигается) разделяются на подклассы по следующим принципам: \n",
    "-использование в процессе поиска экстремума информации о самой  функции, так как в ряде задач целевая функция задана таким образом, что точных значений производных найти нельзя (только оценить);\n",
    "-использование в процессе поиска экстремума информации о самой функции или ее производных;\n",
    "-по виду целевой функции (методы решения одно- и многоэкстремальных задач). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы оптимизации подразделяются на аналитические, и численные (приближенные). Все численные методы решения задач безусловной оптимизации состоят в том, что мы строим последовательность точек таким образом, чтобы последовательность функций была убывающей Последовательность, удовлетворяющая этому условию, называется релаксационной последовательностью. Методы решения делятся на методы с использованием информации о производных функции и без использования таковой.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Численное решение задач безусловной минимизации функций многих переменных, как правило, значительно сложнее, чем решение задач  минимизации функций одного переменного. В самом деле, с ростом числа переменных возрастают объемы вычислений и усложняются конструкции вычислительных алгоритмов, а также более сложным становится анализ поведения целевой функции.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Методы численного решения задач многомерной безусловной минимизации многочисленны и разнообразны. Условно их можно разделить на три больших класса в зависимости от  информации, используемой при реализации метода. \n",
    "1. Методы нулевого порядка, или прямого поиска, стратегия которых основана на использовании информации только о свойствах целевой функции.\n",
    "2. Методы первого порядка, в которых при построении итерационной процедуры наряду с информацией о целевой функции используется информация о значениях первых производных этой функции. \n",
    "3. Методы второго порядка, в которых наряду с информацией о значениях целевой функции и ее производных первого порядка используется информация о вторых производных функции. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы подробно разберем один из методов одномерной оптимизации  с использованием информации о производной: метод касательных или как его еще называют метод Ньютона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод Ньютона (метод касательных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если строго унимодальная на  отрезке [a,b] функция f(x) дважды непре\n",
    "рывно дифференцируема на этом отрезке, то точку ${\\mathbf x^{*}}∊[a,b]$ минимума \n",
    "этой функции можно найти путем решения уравнения f'(x)=0 методом Ньютона, иногда называемым методом касательных. Выбираем ${x_{0}}$ начальное приближение называемое обычно начальной точкой. Линеари\n",
    "зуем функцию f'(x) в окрестности начальной точки, приближенно заме\n",
    "нив  дугу графика этой функции касательной в точке (${x_{0}}$,f'(${x_{0}}$)). Уравнение касательной имеет вид f'(x)≈f'(${x_{0}}$)+f''(${x_{0}}$)*(x-${x_{0}}$). Выберем в \n",
    "качестве следующего приближения к х* точку ${x_{1}}$ пересечения касательной с осью абсцисс. Получаем первый элемент Получаем первый элемент ${x_{1}}$=${x_{0}}$-$\\cfrac{f'{x_{0}}}{f''{x_{0}}}$  итерационной последовательности {${x_{k}}$}. На (k+1)-м шаге по найденной на предыдущем шаге точке {${x_{k}}$} можно найти точку ${x_{k+1}}$=${x_{k}}$-$\\cfrac{f'{x_{k}}}{f''{x_{k}}}$ (1).\n",
    "\n",
    "В общем случае сходимость метода Ньютона существенно зависит от \n",
    "выбора начальной точки ${x_{0}}$. Для надежной работы этого метода необходимо, чтобы вторая производная f''(x) в некоторой окрестности искомой точки х* сохраняла знак, а начальная точка 0 х выбиралась из такой \n",
    "окрестности. В противном случае второе слагаемое в правой части (1) может стать неограниченным.\n",
    "\n",
    "Поскольку для дважды непрерывно дифференцируемой функции в \n",
    "точке минимума f''(x*)>0, то должно быть и f''(${x_{0}}$)>0. Поэтому говорят, что метод Ньютона обладает локальной сходимостью в том смысле, что надо выбрать хорошее начальное приближение, попадающее в такую окрестность точки х*, где f''(x)>0. Однако проверка выполнения этого условия не всегда возможна. Достаточным условием монотонной сходимости метода Ньютона будут постоянство в интервале между точками ${x_{0}}$ и \n",
    "х* знака производной f''(x) и совпадение его со знаком f'(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теорема:\n",
    "\n",
    "Если f(x) трижды дифференцируема в окрестности точки х*, тогда  релаксационная последовательность (1) сходится к х* и имеет место оценка $$|{x_{k+1}}-x*|≤\\cfrac{2max|f'''(x)|}{min|f''(x)|}|{x_{k}}-x*|^{2}$$\n",
    "\n",
    "Доказательство:\n",
    "\n",
    "Рассмотрим формулу Тейлора для f'(x*) в окрестности точки ${x_{k}}$, учитывая, что f'(x*)=0 $$0=f'(x*)=f'({x_{k}})+f''({x_{k}})(x*-{x_{k}})+\\cfrac{f'''(ξ)}{2}(x*-{x_{k}})^2 , ξ∊({x_{k}}-x*)$$\n",
    "\n",
    "Откуда получаем $$ f''({x_{k}})(x*-{x_{k}})=-f'({x_{k}})-\\cfrac{f'''(ξ)}{2}(x*-{x_{k}})^2   (2)$$\n",
    "\n",
    "Рассмотрим отношение разностей приближенного и точного решения на k-м и (k+1)-м шаге, подставив (1) и (2)\n",
    "$$\\cfrac{x*-{x_{k+1}}}{x*-{x_{k}}}=\\cfrac{x*-{x_{k}}+\\cfrac{f'({x_{k}})}{f''({x_{k}})}}{x*-{x_{k}}}=1+\\cfrac{f'({x_{k}})}{f''({x_{k}})(x*-{x_{k}})}$$\n",
    "\n",
    "Оттуда получаем, что если $\\cfrac{f'''(ξ)}{f'{x_{k}}}>0$, то\n",
    "$$\\cfrac{x*-{x_{k+1}}}{x*-{x_{k}}}=1-\\cfrac{2}{2f'({x_{k}})+\\cfrac{f'''(ξ)}{f'{x_{k+1}}}(x*-{x_{k}})^2}<1$$\n",
    "\n",
    "следовательно, последовательность {${x_{k}}$} монотонно убывает и ограничена и ${lim_{k→∞}}|{x_{k}}-x^*|$=0. Далее оценим скорость сходимости метода , из (2) получаем $$\\cfrac{f'({x_{k}})}{f''({x_{k}})}=-(x^*-{x_{k}})-\\cfrac{f'''({x_{k}})}{2f''({x_{k}})}(x^*-{x_{k}})^2$$ тогда \n",
    "$$|{x_{k+1}}-x^*|=|{x_{k}}-x^*-\\cfrac{f'({x_{k}})}{f''({x_{k}})}|=|\\cfrac{f'''(ξ)}{2f''({x_{k}})}|({x_{k}}-x^*)^2$$\n",
    "\n",
    "Из последнего равенства следует оценка теоремы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
