{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы безусловной глобальной оптимизации. Метод Ньютона для оптимизации. Метод BGFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизация (от латинского слова «optimus» – наилучший) – поиск наилучшего варианта, при наличии множества альтернативных. Всевозможные устройства, процессы и ситуации, применительно к которым предстоит решать задачу оптимизации, называют объектом оптимизации. \n",
    "\n",
    "Методы оптимизации занимаются построением оптимальных решений для математических моделей, при этом именно вид модели определяет метод или методы, используемые для построения оптимального решения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Классификация задач оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу оптимизации: найти минимум функции f(x) n-мерного аргумента x, компоненты которого удовлетворяют системе ограничений в виде уравнений или неравенств.Такая задача называется задачей условной оптимизации. Если задача не содержит ограничения и рассматривается на всем пространстве, то это задача безусловной оптимизации.                                                                                  \n",
    "\n",
    "Задачи оптимизации классифицируются в соответствии с видом функций и размерностью вектора х.                                                                                                              \n",
    "\n",
    "Задачи без ограничений с N=1 называются задачами одномерной оптимизации, с N=2 и N>2 – многомерной оптимизации.                                                                                                    Если в задаче функции линейны, то это задача с линейными ограничениями. При этом целевая функция f(x) может быть как линейной, так и нелинейной.  Задача условной оптимизации, в которой все функции линейны, называется задачей линейного программирования. \n",
    "\n",
    "Задачи с нелинейной целевой функцией называются задачами нелинейного программирования. При этом если f(x) квадратичная функция, то задачей квадратичного программирования. Если f(x) отношение линейных функций, а ограничения – линейные, то рассматривается задача дробнолинейного программирования.                                                                                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Классификация методов оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В соответствии с классификацией задач оптимизации классифицируются и методы оптимизации. Методы одномерной оптимизации (нахождение наименьшего значения функции f(x) в области определения D(f) и точек, в которых это значение достигается) разделяются на подклассы по следующим принципам: \n",
    "-использование в процессе поиска экстремума информации о самой  функции, так как в ряде задач целевая функция задана таким образом, что точных значений производных найти нельзя (только оценить);\n",
    "-использование в процессе поиска экстремума информации о самой функции или ее производных;\n",
    "-по виду целевой функции (методы решения одно- и многоэкстремальных задач). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы оптимизации подразделяются на аналитические, и численные (приближенные). Все численные методы решения задач безусловной оптимизации состоят в том, что мы строим последовательность точек таким образом, чтобы последовательность функций была убывающей Последовательность, удовлетворяющая этому условию, называется релаксационной последовательностью. Методы решения делятся на методы с использованием информации о производных функции и без использования таковой.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Численное решение задач безусловной минимизации функций многих переменных, как правило, значительно сложнее, чем решение задач  минимизации функций одного переменного. В самом деле, с ростом числа переменных возрастают объемы вычислений и усложняются конструкции вычислительных алгоритмов, а также более сложным становится анализ поведения целевой функции.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Методы численного решения задач многомерной безусловной минимизации многочисленны и разнообразны. Условно их можно разделить на три больших класса в зависимости от  информации, используемой при реализации метода. \n",
    "1. Методы нулевого порядка, или прямого поиска, стратегия которых основана на использовании информации только о свойствах целевой функции.\n",
    "2. Методы первого порядка, в которых при построении итерационной процедуры наряду с информацией о целевой функции используется информация о значениях первых производных этой функции. \n",
    "3. Методы второго порядка, в которых наряду с информацией о значениях целевой функции и ее производных первого порядка используется информация о вторых производных функции. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы подробно разберем один из методов одномерной оптимизации  с использованием информации о производной: метод касательных или как его еще называют метод Ньютона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод Ньютона (метод касательных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод Ньютона является одним из самых распространенных методов решения систем\n",
    "нелинейных алгебраических уравнений. Это итерационный численный метод нахождения корня (нуля) заданной функции.  Поиск решения осуществляется путём построения последовательных приближений и основан на принципах простой итерации. Метод обладает квадратичной сходимостью. Модификацией метода является метод хорд и касательных. Также метод Ньютона может быть использован для решения задач оптимизации, в которых требуется определить ноль первой производной либо градиента в случае многомерного пространства. \n",
    "\n",
    "Теорема:\n",
    "\n",
    "Если строго унимодальная на  отрезке [a,b] функция f(x) дважды непрерывно дифференцируема на этом отрезке, то точку ${\\mathbf x^{*}}∊[a,b]$ минимума этой функции можно найти путем решения уравнения f'(x)=0 методом Ньютона, иногда называемым методом касательных. \n",
    "\n",
    "Доказательство:\n",
    "\n",
    "Выбираем ${x_{0}}$ начальное приближение называемое обычно начальной точкой. Линеаризуем функцию f'(x) в окрестности начальной точки, приближенно заменив  дугу графика этой функции касательной в точке (${x_{0}}$,f'(${x_{0}}$)). Уравнение касательной имеет вид f'(x)≈f'(${x_{0}}$)+f''(${x_{0}}$)*(x-${x_{0}}$). Выберем в \n",
    "качестве следующего приближения к х* точку ${x_{1}}$ пересечения касательной с осью абсцисс. Получаем первый элемент Получаем первый элемент ${x_{1}}$=${x_{0}}$-$\\cfrac{f'{x_{0}}}{f''{x_{0}}}$  итерационной последовательности {${x_{k}}$}. На (k+1)-м шаге по найденной на предыдущем шаге точке {${x_{k}}$} можно найти точку ${x_{k+1}}$=${x_{k}}$-$\\cfrac{f'{x_{k}}}{f''{x_{k}}}$ (1).\n",
    "\n",
    "В общем случае сходимость метода Ньютона существенно зависит от \n",
    "выбора начальной точки ${x_{0}}$. Для надежной работы этого метода необходимо, чтобы вторая производная f''(x) в некоторой окрестности искомой точки х* сохраняла знак, а начальная точка 0 х выбиралась из такой \n",
    "окрестности. В противном случае второе слагаемое в правой части (1) может стать неограниченным.\n",
    "\n",
    "Поскольку для дважды непрерывно дифференцируемой функции в \n",
    "точке минимума f''(x*)>0, то должно быть и f''(${x_{0}}$)>0. Поэтому говорят, что метод Ньютона обладает локальной сходимостью в том смысле, что надо выбрать хорошее начальное приближение, попадающее в такую окрестность точки х*, где f''(x)>0. Однако проверка выполнения этого условия не всегда возможна. Достаточным условием монотонной сходимости метода Ньютона будут постоянство в интервале между точками ${x_{0}}$ и \n",
    "х* знака производной f''(x) и совпадение его со знаком f'(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теорема:\n",
    "\n",
    "Если f(x) трижды дифференцируема в окрестности точки х*, тогда  релаксационная последовательность (1) сходится к х* и имеет место оценка $$|{x_{k+1}}-x^*|≤\\cfrac{2max|f'''(x)|}{min|f''(x)|}|{x_{k}}-x^*|^{2}$$\n",
    "\n",
    "Доказательство:\n",
    "\n",
    "Рассмотрим формулу Тейлора для f'(x*) в окрестности точки ${x_{k}}$, учитывая, что f'(x*)=0 $$0=f'(x^*)=f'({x_{k}})+f''({x_{k}})(x^*-{x_{k}})+\\cfrac{f'''(ξ)}{2}(x^*-{x_{k}})^2 , ξ∊({x_{k}}-x^*)$$\n",
    "\n",
    "Откуда получаем $$ f''({x_{k}})(x^*-{x_{k}})=-f'({x_{k}})-\\cfrac{f'''(ξ)}{2}(x^*-{x_{k}})^2   (2)$$\n",
    "\n",
    "Рассмотрим отношение разностей приближенного и точного решения на k-м и (k+1)-м шаге, подставив (1) и (2)\n",
    "$$\\cfrac{x^*-{x_{k+1}}}{x^*-{x_{k}}}=\\cfrac{x^*-{x_{k}}+\\cfrac{f'({x_{k}})}{f''({x_{k}})}}{x*-{x_{k}}}=1+\\cfrac{f'({x_{k}})}{f''({x_{k}})(x^*-{x_{k}})}$$\n",
    "\n",
    "Оттуда получаем, что если $\\cfrac{f'''(ξ)}{f'{x_{k}}}>0$, то\n",
    "$$\\cfrac{x^*-{x_{k+1}}}{x^*-{x_{k}}}=1-\\cfrac{2}{2f'({x_{k}})+\\cfrac{f'''(ξ)}{f'{x_{k+1}}}(x^*-{x_{k}})^2}<1$$\n",
    "\n",
    "следовательно, последовательность {${x_{k}}$} монотонно убывает и ограничена и ${lim_{k→∞}}|{x_{k}}-x^*|$=0. Далее оценим скорость сходимости метода , из (2) получаем $$\\cfrac{f'({x_{k}})}{f''({x_{k}})}=-(x^*-{x_{k}})-\\cfrac{f'''({x_{k}})}{2f''({x_{k}})}(x^*-{x_{k}})^2$$ тогда \n",
    "$$|{x_{k+1}}-x^*|=|{x_{k}}-x^*-\\cfrac{f'({x_{k}})}{f''({x_{k}})}|=|\\cfrac{f'''(ξ)}{2f''({x_{k}})}|({x_{k}}-x^*)^2$$\n",
    "\n",
    "Из последнего равенства следует оценка теоремы.\n",
    "\n",
    "Отметим важные особенности метода Ньютона:\n",
    "1. начальное приближение ${x_{0}}$ должно быть достаточно близким к корню x*;\n",
    "2. градиент f'(${x_{k}}$) должен быть отличен от нуля для любого k. Более того, метод Ньютона тем эффективнее, чем дальше f'(x*) от нуля;\n",
    "3. является “оптимальным” методом простой итерации и имеет квадратичную сходимость.\n",
    "\n",
    "\n",
    "\n",
    "Геометрическая интерпретация\n",
    "\n",
    "Основная идея метода заключается в следующем: задаётся начальное приближение вблизи предположительного корня, после чего строится касательная к графику исследуемой функции в точке приближения, для которой находится пересечение с осью абсцисс. Эта точка берётся в качестве следующего приближения. И так далее, пока не будет достигнута необходимая точность.\n",
    "\n",
    "\n",
    "\n",
    "Пусть \n",
    "1) вещественнозначная функция $f(x)\\colon(a,\\,b)\\to\\R$ непрерывно дифференцируемая функция на интервале $(a,\\,b) $;\n",
    "2) существует искомая точка $x^{*}\\in (a,\\,b)$ : $f(x^{*})= 0$;\n",
    "3) существуют C > 0 и $\\delta > 0$ такие, что $\\vert f'(x) \\vert \\geqslant C$ для $x\\in(a,\\,x^{*}-\\delta ] \\cup [ x^{*}+\\delta,\\,b)$ и $f'(x)\\ne 0$ для $x\\in(x^{*}-\\delta,\\, x^{*})\\cup(x^{*},\\, x^{*}+\\delta )$ ; \n",
    "4) точка $x_n \\in (a,\\,b)$ такова, что $f(x_n)\\ne 0$.\n",
    "\n",
    "Тогда формула итеративного приближения $x_n$ к $x^{*}$ может быть выведена из геометрического смысла касательной следующим образом:\n",
    ": $f'(x_n)=\\mathrm{tg}\\,\\alpha_{n} = \\frac{\\Delta y}{\\Delta x} = \\frac{f(x_n)-0}{x_n-x_{n+1}} = \\frac{0-f(x_n)}{x_{n+1}-x_n},$\n",
    "где $\\alpha_{n}$ — угол наклона касательной прямой\n",
    "$y(x)=f(x_n) + (x - x_n) \\cdot \\mathrm{tg}\\,\\alpha_{n}$\n",
    "к графику f\n",
    "в точке $(x_n;f(x_n))$.\n",
    "\n",
    "Следовательно (в уравнении касательной прямой полагаем $y(x_{n+1})=0$) искомое выражение для $x_{n+1}$ имеет вид :\n",
    ": $x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}.$\n",
    "Если $x_{n+1} \\in (a,\\,b)$, то это значение можно использовать в качестве следующего приближения к\n",
    "$x^{*}$ .\n",
    "\n",
    "Если $x_{n+1} \\notin (a,\\,b)$, то имеет место «перелёт» (корень $x^{*}$ лежит рядом с границей $(a,\\,b)$). В этом случае надо (воспользовавшись идеей метода половинного деления) заменять $x_{n+1}$ на\n",
    "$\\frac{x_{n}+x_{n+1}}{2}$\n",
    "до тех пор, пока точка «не вернётся» в область поиска\n",
    "$(a,\\,b)$ .\n",
    "\n",
    "\n",
    "\n",
    "Замечания\n",
    "\n",
    "1) Наличие непрерывной производной даёт возможность строить непрерывно меняющуюся касательную на всей области поиска решения $(a,\\,b)\\;$ . <br>\n",
    "2) Случаи граничного (в точке a или в точке b) расположения искомого решения $x^{*}$ рассматриваются аналогичным образом. <br>\n",
    "3) С геометрической точки зрения равенство\n",
    "$f'(x_n)= 0$\n",
    "означает, что касательная прямая к графику f в точке\n",
    "$(x_n;f(x_n))$ - параллельна оси OX и при\n",
    "$f(x_n)\\ne 0$ не пересекается с ней в конечной части. <br>\n",
    "4) Чем больше константа C > 0 и чем меньше константа $\\delta > 0$ из пункта 3 условий,\n",
    "тем для $x_{n}\\in(a,\\,x^{*}-\\delta ] \\cup [ x^{*}+\\delta,\\,b)$\n",
    "пересечение касательной к графику f и оси OX ближе к точке $(x^{*};\\;0)$,\n",
    "то есть тем ближе значение $x_{n+1}$ к искомой $x^{*}\\in (a,\\,b)$ .\n",
    "\n",
    "Итерационный процесс начинается с некоторого начального приближения $x_0 \\in (a,\\,b)$ , причём между $x_0 \\in (a,\\,b)$ и искомой точкой $x^{*}\\in (a,\\,b)$ не должно быть других нулей функции f, то есть «чем ближе $x_0$ к искомому корню $x^{*}$ , тем лучше». Если предположения о нахождении $x^{*}$ отсутствуют, методом проб и ошибок можно сузить область возможных значений, применив теорему о промежуточных значениях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостаток:\n",
    "Метод Ньютона для многомерного случая требует на каждй итерации n вычислений значений векторной функции $f({x_{k-1}})$, $n^2$ вычислений матрицы Якоби (матрицы частных производных) $J({x_{k-1}})$ и $O(n^3)$ операций для нахлждения $y({x_{k-1}})$. Подобная алгоритмическая сложность не позволяет использовать метод Ньютона в его стандартной форме для систем большой размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Квазиньютоновские методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Квазиньютоновские методы представляют собой класс методов, уменьшающих алгоритмическую сложность метода Ньютона за счет аппроксимации матрицы Якоби тем\n",
    "или иных способом. Уменьшение алгоритмической сложности, однако, приводит к замене\n",
    "квадратичной сходимости на сверхлинейную. Более того, так как аппроксимация матрицы Якоби в сущности является заменой точных производных скалярных функций на аппроксимации путем численного дифференцирования, квазиньютоновские методы являются вычислительно неустойчивыми и могут накапливать погрешность округления с каждой\n",
    "итерацией.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разложим градиент $\\vec{g}(\\vec{x}_k)$ исходной функции в ряд Тейлора в окрестности точки очередного приближения $\\vec{x}_k$ по степеням следующего шага алгоритма $\\vec{s}_k$:\n",
    "\n",
    "$$\\vec{g}(\\vec{x}_k+\\vec{s}_k)\\approx \\vec{g}(\\vec{x}_k)+G(\\vec{x}_k)\\vec{s}_k$$\n",
    "\n",
    "Тогда оценка матрицы Гессе $B_{k+1}$ должна удовлетворять равенству:\n",
    "\n",
    "$$B_{k+1}\\vec{s}_k=\\vec{y}_k$$\n",
    "\n",
    "где $$\\vec{y}_k=\\vec{g}(\\vec{x}_k+\\vec{s}_k)- \\vec{g}(\\vec{x}_k)$$\n",
    "\n",
    "это условие называют ''квазиньютоновским''.\n",
    "Матрица Гессе — это квадратная матрица частных производных второго порядка скалярнозначной функции или скалярного поля. Она описывает локальную кривизну функции многих переменных. \n",
    "Определитель этой матрицы называется определителем Гессе или просто гессианом. \n",
    "\n",
    "\n",
    "\n",
    "На каждой итерации с помощью $B_k$ определяется следующее направление поиска $\\vec{p}_k$, и матрица B обновляется с учётом вновь полученной информации о кривизне:\n",
    "\n",
    "$$B_k\\vec{p}_k=-\\vec{g}(\\vec{x}_k)$$\n",
    "\n",
    "$$B_{k+1}=B_k+U_k$$\n",
    "\n",
    "где $U_k$ — матрица, характеризующая поправку, вносимую на очередном шаге.\n",
    "\n",
    "В качестве начального приближения $B_0$ кладут единичную матрицу, таким образом первое направление $\\vec{p}_0$ будет в точности совпадать с направлением наискорейшего спуска.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поправка единичного ранга:\n",
    "\n",
    "Один шаг алгоритма даёт информацию о кривизне вдоль одного направления, поэтому ранг матрицы $U_k$ полагают малым, и даже единичным:\n",
    "\n",
    "$$B_{k+1}=B_k+\\vec{u}\\vec{v}^T$$\n",
    "\n",
    "где $\\vec{u}$ и $\\vec{v}$ некоторые вектора.\n",
    "\n",
    "Тогда, квазиньютоновское условие примет вид:\n",
    "\n",
    "$$(B_k+\\vec{u}\\vec{v}^T)\\vec{s}_k=\\vec{y}_k$$\n",
    "\n",
    "$$\\vec{u}(\\vec{v}^T\\vec{s}_k)=\\vec{y}_k-B_k\\vec{s}_k$$\n",
    "\n",
    "Полагая, что предыдущая матрица $B_k$ на очередном шаге квазиньютоновскому условию не удовлетворяет (т.е. разность в правой части не равна нулю), и что вектор $\\vec{v}$ не ортогонален $\\vec{s}_k$, получают выражение для $\\vec{u}$ и $B_{k+1}$:\n",
    "\n",
    "$$\\vec{u}=\\frac{1}{\\vec{v}^T\\vec{s}_k}(\\vec{y}_k-B_k\\vec{s}_k)$$\n",
    "\n",
    "$$B_{k+1}=B_k+\\frac{1}{\\vec{v}^T\\vec{s}_k}(\\vec{y}_k-B_k\\vec{s}_k)\\vec{v}^T$$\n",
    "\n",
    "Из соображений симметричности матрицы Гессе, вектор $\\vec{v}$ берут коллинеарным $\\vec{u}$\n",
    "\n",
    "$$B_{k+1}=B_k+\\frac{1}{(\\vec{y}_k-B_k\\vec{s}_k)^T\\vec{s}_k}(\\vec{y}_k-B_k\\vec{s}_k)(\\vec{y}_k-B_k\\vec{s}_k)^T$$\n",
    "\n",
    "Полученное уравнение называется симметричной формулой ранга один."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поправки ранга два:\n",
    "\n",
    "\n",
    "Один из способов конструирования поправок ранга два заключается в построении сходящейся последовательности матриц $B^{(j)}$. В качестве начального значения $B^{(0)}$ берут $B_k$, $B^{(1)}$ вычисляют по формуле:\n",
    "\n",
    "$$B^{(1)}=B^{(0)}+\\frac{1}{\\vec{v}^T\\vec{s}_k}(\\vec{y}_k-B^{(0)}\\vec{s}_k)\\vec{v}^T$$\n",
    "\n",
    "После чего её симметризуют:\n",
    "\n",
    "$$B^{(2)}=\\frac{B^{(1)}+B^{(1)T}}{2}$$\n",
    "\n",
    "Однако полученная матрица больше не удовлетворяет квазиньютоновскому условию. Чтобы это исправить, процедуру повторяют. В результате на j-м шаге:\n",
    "\n",
    "$$B^{(2j+1)}=B^{(2j)}+\\frac{1}{\\vec{v}^T\\vec{s}_k}(\\vec{y}_k-B^{(2j)}\\vec{s}_k)\\vec{v}^T$$\n",
    "$$B^{(2j+2)}=\\frac{B^{(2j+1)}+B^{(2j+1)T}}{2}$$\n",
    "\n",
    "Предел этой последовательности равен:\n",
    "\n",
    "$$B_{k+1}=B_k+\\frac{1}{\\vec{v}^T\\vec{s}_k}[(\\vec{y}_k-B_k\\vec{s}_k)\\vec{v}^T+\\vec{v}(\\vec{y}_k-B_k\\vec{s}_k)^T]-\\frac{(\\vec{y}_k-B_k\\vec{s}_k)^T\\vec{s}_k}{(\\vec{v}^T\\vec{s}_k)^2}\\vec{v}\\vec{v}^T$$\n",
    "\n",
    "При выборе различных $\\vec{v}$ (не ортогональных $\\vec{s}_k$) получаются различные формулы пересчёта матрицы B:\n",
    "* $\\vec{v}=\\vec{y}_k-B_k\\vec{s}_k$ приводит к '''симметричной формуле ранга один''';\n",
    "* $\\vec{v}=\\vec{s}_k$ приводит к '''симметричной формуле Пауэлла — Бройдена (PSB)''';\n",
    "* $\\vec{v}=\\vec{y}_k$ приводит к '''симметричной формуле Девидона — Флетчера — Пауэлла (DFP)''':\n",
    "\n",
    "$$B_{k+1}=B_k-\\frac{1}{\\vec{s}_k^T B_k \\vec{s}_k}B_k \\vec{s}_k \\vec{s}_k^T B_k^T+\\frac{1}{\\vec{y}_k^T\\vec{s}_k}\\vec{y}_k\\vec{y}_k^T+(\\vec{s}_k^T B_k \\vec{s}_k)\\vec{\\omega}_k\\vec{\\omega}_k^T$$\n",
    "\n",
    "где $$\\vec{\\omega}_k=\\frac{1}{\\vec{y}_k^T\\vec{s}_k}\\vec{y}_k-\\frac{1}{\\vec{s}_k^T B_k \\vec{s}_k}B_k \\vec{s}_k$$\n",
    "\n",
    "Нетрудно проверить, что $\\vec{\\omega}_k$ ортогонален $\\vec{s}_k$. Таким образом добавление слагаемого $\\vec{\\omega}_k\\vec{\\omega}_k^T$ не нарушит ни квазиньютоновского условия, ни условия симметричности. Поэтому проводился ряд теоретических исследований, подвергавших последнее слагаемое масштабированию на предмет получения наилучшего приближения. В результате была принята точка зрения, что наилучшим вариантом является отвечающий полному отсутствию последнего слагаемого. Этот вариант пересчёта известен под именем формулы Бройдена — Флетчера — Гольдфарба — Шанно (BFGS):\n",
    "\n",
    "$$B_{k+1}=B_k-\\frac{1}{\\vec{s}_k^T B_k \\vec{s}_k}B_k \\vec{s}_k \\vec{s}_k^T B_k^T+\\frac{1}{\\vec{y}_k^T\\vec{s}_k}\\vec{y}_k\\vec{y}_k^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практическое задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите минимальное значение функции одной переменной с использованием метода Ньютона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сошлись за 2 итераций.\n",
      "Оптимальное решение: 3.0\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "# Определим функцию, которую будем минимизировать\n",
    "function f(x)\n",
    "    return (x - 3)^2 + 5\n",
    "end\n",
    "\n",
    "# Определим производную функции (градиент)\n",
    "function grad_f(x)\n",
    "    return 2 * (x - 3)\n",
    "end\n",
    "\n",
    "# Определим вторую производную функции (гессиан)\n",
    "function hess_f(x)\n",
    "    return 2\n",
    "end\n",
    "\n",
    "# Реализация метода Ньютона\n",
    "function newton_method(x0; tol=1e-6, max_iter=100)\n",
    "    x = x0\n",
    "    for i in 1:max_iter\n",
    "        g = grad_f(x)\n",
    "        H = hess_f(x)\n",
    "\n",
    "        # Проверка, что гессиан не равен нулю\n",
    "        if H == 0\n",
    "            println(\"Гессиан равен нулю. Метод не может продолжаться.\")\n",
    "            return x\n",
    "        end\n",
    "\n",
    "        # Решаем систему H * p = -g для нахождения направления спуска\n",
    "        p = -g / H\n",
    "\n",
    "        # Обновляем x\n",
    "        x += p\n",
    "\n",
    "        # Проверяем условие остановки\n",
    "        if abs(g) < tol\n",
    "            println(\"Сошлись за $i итераций.\")\n",
    "            return x\n",
    "        end\n",
    "    end\n",
    "    println(\"Достигнуто максимальное количество итераций.\")\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Начальная точка\n",
    "x0 = 0.0\n",
    "\n",
    "# Запуск метода Ньютона\n",
    "optimal_x = newton_method(x0)\n",
    "println(\"Оптимальное решение: \", optimal_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте оптимизацию функции двух переменных с использованием метода Ньютона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сошлись за 2 итераций.\n",
      "Оптимальное решение: [1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "# Определим функцию, которую будем минимизировать\n",
    "function f(x)\n",
    "    return (x[1] - 1)^2 + (x[2] - 2)^2\n",
    "end\n",
    "\n",
    "# Определим градиент функции\n",
    "function grad_f(x)\n",
    "    return [2 * (x[1] - 1), 2 * (x[2] - 2)]\n",
    "end\n",
    "\n",
    "# Определим гессиан функции\n",
    "function hess_f(x)\n",
    "    return [2 0; 0 2]\n",
    "end\n",
    "\n",
    "# Реализация метода Ньютона\n",
    "function newton_method(x0; tol=1e-6, max_iter=100)\n",
    "    x = x0\n",
    "    for i in 1:max_iter\n",
    "        g = grad_f(x)\n",
    "        H = hess_f(x)\n",
    "\n",
    "        # Решаем систему H * p = -g для нахождения направления спуска\n",
    "        p = -H \\ g\n",
    "\n",
    "        # Обновляем x\n",
    "        x += p\n",
    "\n",
    "        # Проверяем условие остановки\n",
    "        if norm(g) < tol\n",
    "            println(\"Сошлись за $i итераций.\")\n",
    "            return x\n",
    "        end\n",
    "    end\n",
    "    println(\"Достигнуто максимальное количество итераций.\")\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Начальная точка\n",
    "x0 = [0.0, 0.0]\n",
    "\n",
    "# Запуск метода Ньютона\n",
    "optimal_x = newton_method(x0)\n",
    "println(\"Оптимальное решение: \", optimal_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте оптимизацию функции четырех переменных с использованием метода Ньютона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сошлись за 2 итераций.\n",
      "Оптимальное решение: [1.0, 2.0, 3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "# Определим функцию, которую будем минимизировать\n",
    "function f(x)\n",
    "    return (x[1] - 1)^2 + (x[2] - 2)^2 + (x[3] - 3)^2 + (x[4] - 4)^2\n",
    "end\n",
    "\n",
    "# Определим градиент функции\n",
    "function grad_f(x)\n",
    "    return 2 * (x .- [1, 2, 3, 4])\n",
    "end\n",
    "\n",
    "# Определим гессиан функции\n",
    "function hess_f(x)\n",
    "    return 2 * I(4)  # 2 * единичная матрица размером 4x4\n",
    "end\n",
    "\n",
    "# Реализация метода Ньютона\n",
    "function newton_method(x0; tol=1e-6, max_iter=100)\n",
    "    x = x0\n",
    "    for i in 1:max_iter\n",
    "        g = grad_f(x)\n",
    "        H = hess_f(x)\n",
    "\n",
    "        # Проверка, что гессиан не равен нулю\n",
    "        if det(H) == 0\n",
    "            println(\"Гессиан вырождён. Метод не может продолжаться.\")\n",
    "            return x\n",
    "        end\n",
    "\n",
    "        # Решаем систему H * p = -g для нахождения направления спуска\n",
    "        p = -H \\g   # Используем оператор обратной матрицы\n",
    "\n",
    "        # Обновляем x\n",
    "        x += p\n",
    "\n",
    "        # Проверяем условие остановки\n",
    "        if norm(g) < tol\n",
    "            println(\"Сошлись за $i итераций.\")\n",
    "            return x\n",
    "        end\n",
    "    end\n",
    "    println(\"Достигнуто максимальное количество итераций.\")\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Начальная точка\n",
    "x0 = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "# Запуск метода Ньютона\n",
    "optimal_x = newton_method(x0)\n",
    "println(\"Оптимальное решение: \", optimal_x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
